{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "dataset = dataset.remove_columns(['article', 'id'])  # remove the 'article' and 'id' fields as we will not be using them in our task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.random import default_rng\n",
        "\n",
        "rng = default_rng(seed=42)\n",
        "# create a list of non-repeated indices of size 10000 and use it to select the training samples\n",
        "select_idx = rng.choice(len(dataset), size=10000, replace=False)\n",
        "dataset = dataset.filter(lambda example, idx: idx in select_idx, with_indices=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['highlights'],\n",
              "    num_rows: 10000\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n",
              " \"David Nalbandian won Madrid Masters after beating top seed Roger Federer .\\nThe Argentine triumphed 1-6 6-3 6-3 against the Swiss defending champion .\\nHe became third man to beat world's three top players en route to a title .\",\n",
              " 'Bus driver, tractor-trailer trucker injured in 3-vehicle crash, AP reports .\\nBus crossed median, hit pickup truck, then 18-wheeler, killing 3 .\\nAll lanes of I-40 closed for 13 miles east of Forrest City, Arkansas .\\nDriver of the pickup truck and two bus passengers killed, police say .',\n",
              " 'North Korean crew recaptures hijacked vessel .\\nNavy says two pirates killed, five captured; three from crew injured .\\nUSS Arleigh Burke enters Somali territorial waters to pursue other pirates .\\nPirates aboard hijacked Golden Nori carrying highly flammable benzene .',\n",
              " 'Detainees file suit against ICE, allege they were injected with drugs .\\nOne detainee tells CNN: \"What they did to me was very, very bad\"\\nICE says 1,073 detainees have had \"medical escorts\" since 2003 .\\nSen. Lieberman vows to investigate further; spokesman calls charges \"troubling\"',\n",
              " \"Glasgow derby between Celtic and Rangers on Wednesday is postponed .\\nDecision made as a mark of respect after death of ex-player Phil O'Donnell .\\nMotherwell's home game against Celtic next Sunday was already called off .\",\n",
              " 'Head of Iraqi Red Crescent says parents abandoning their kids at alarming rates .\\nSometimes he wonders: \"Oh my God, how are we going to solve it?\"\\nThe greatest concern is the long-term effect on an entire generation .\\n\"Trauma of what\\'s happening to those children is enormous\"',\n",
              " \"At least 10 people have died in torrential rains in Ecuador, officials say .\\nAuthorities say the rains are the worst in a quarter century .\\nCivil defense officials say more than 10,000 families have been affected .\\nEcuador's president declares state of emergency, orders army, police to help .\",\n",
              " 'Democratic senators will hold short \"pro forma\" sessions over the holiday break .\\nSessions usually under one minute long .\\nMove prevents President Bush from making recess appointments .\\nBush refused to withdraw one controversial nominee after Democratic offer .',\n",
              " 'GAO: Investigators took advantage of \"weaknesses in TSA procedures\"\\nGAO: Those \"weaknesses\" were not detailed for security reasons .\\nGAO said terrorists using same items could seriously compromise a flight .\\nTransportation Security Administration: Checkpoints only part of total security .']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's check the first articles\n",
        "dataset['highlights'][:10]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "### Tokenization and numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the prefix \"xxxxx -- \" from all article text\n",
        "\n",
        "sentences = [x.replace(\"\\n\", \" \") for x in dataset['highlights']]\n",
        "sentences = [x for x in sentences if len(x.split()) <= 500]  # remove any text which, after has tokens length > 500\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lower case, and clean all the symbols\n",
        "sentences = [x.lower() for x in sentences]\n",
        "sentences = [re.sub(\"[.,!?\\\\-]\", '', x) for x in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#making vocabs - numericalization\n",
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, w in enumerate(word_list):\n",
        "    word2id[w] = i + 4 #reserve the first 0-3 for CLS, PAD\n",
        "    \n",
        "vocab_size = len(word2id)\n",
        "id2word    = {i:w for i, w  in enumerate(word2id)}\n",
        "token_list = [[word2id[word] for word in sentence.split()] for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(token_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loader\n",
        "\n",
        "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
        "\n",
        "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
        "\n",
        "2. **Segment embedding**\n",
        "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
        "\n",
        "3. **Masking**\n",
        "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
        "\n",
        "4. **Padding**\n",
        "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
        "\n",
        "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_batch(batch_size, max_mask, max_len):\n",
        "    batch = []\n",
        "    half_batch_size = batch_size // 2\n",
        "    positive = negative = 0\n",
        "    while positive != half_batch_size or negative != half_batch_size:\n",
        "        \n",
        "        #randomly choose two sentence\n",
        "        tokens_a_index, tokens_b_index = np.random.randint(len(sentences), size=2)\n",
        "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "        \n",
        "        #1. token embedding - add CLS and SEP\n",
        "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
        "        \n",
        "        #2. segment embedding - which sentence is 0 and 1\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "        \n",
        "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get all the pos excluding CLS and SEP\n",
        "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
        "                                 and token != word2id['[SEP]']]\n",
        "        np.random.shuffle(candidates_masked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        #simply loop and mask accordingly\n",
        "        for pos in candidates_masked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            rand_val = np.random.random()\n",
        "            if rand_val < 0.1:  #10% replace with random token\n",
        "                index = np.random.randint(4, vocab_size - 1)  # random token should not involve [PAD], [CLS], [SEP], [MASK]\n",
        "                input_ids[pos] = word2id[id2word[index]]\n",
        "            elif rand_val < 0.8:  #80 replace with [MASK]\n",
        "                input_ids[pos] = word2id['[MASK]']\n",
        "            else: \n",
        "                pass\n",
        "            \n",
        "        #4. pad the sentence to the max length\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "        \n",
        "        #5. pad the mask tokens to the max length\n",
        "        if max_mask > n_pred:\n",
        "            n_pad = max_mask - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "        \n",
        "        #6. check whether is positive or negative\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < half_batch_size:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < half_batch_size:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "        \n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "max_mask   = 15 #even though it does not reach 15% yet....maybe you can set this threshold\n",
        "max_len    = 1024 #maximum length that my transformer will accept.....all sentence will be padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = make_batch(batch_size, max_mask, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 1024]),\n",
              " torch.Size([8, 1024]),\n",
              " torch.Size([8, 15]),\n",
              " torch.Size([8, 15]),\n",
              " tensor([0, 0, 0, 0, 1, 1, 1, 1]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n",
        "\n",
        "Recall that BERT only uses the encoder.\n",
        "\n",
        "BERT has the following components:\n",
        "\n",
        "- Embedding layers\n",
        "- Attention Mask\n",
        "- Encoder layer\n",
        "- Multi-head attention\n",
        "- Scaled dot product attention\n",
        "- Position-wise feed-forward network\n",
        "- BERT (assembling all the components)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Embedding\n",
        "\n",
        "<img src = \"../figures/BERT_embed.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k, device):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 1024, 1024])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids, 'cpu').shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Encoder\n",
        "\n",
        "The encoder has two main components: \n",
        "\n",
        "- Multi-head Attention\n",
        "- Position-wise feed-forward network\n",
        "\n",
        "First let's make the wrapper called `EncoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the scaled dot attention, to be used inside the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, device):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the Multiheadattention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, device):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_k\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
        "        self.device = device\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
        "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the PoswiseFeedForwardNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
        "        super(BERT, self).__init__()\n",
        "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
        "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
        "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
        "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp\n",
        "    \n",
        "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the parameters first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:35<00:00,  1.56s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 4\n",
        "max_mask   = 15\n",
        "max_len    = 1024\n",
        "num_epoch = 1\n",
        "num_step = 100  # 100 num_steps * 4 batch_size = 400 pairs of sentences\n",
        "\n",
        "model = BERT(n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "batch = [make_batch(batch_size, max_mask, max_len) for i in tqdm(range(num_step))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:39<00:00,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 loss = 61.766463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epoch):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step in tqdm(range(num_step)):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch[step]))\n",
        "        input_ids = input_ids.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        masked_tokens = masked_tokens.to(device)\n",
        "        masked_pos = masked_pos.to(device)\n",
        "        isNext = isNext.to(device)\n",
        "        logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "        #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "        #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "        #1. mlm loss\n",
        "        #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        #2. nsp loss\n",
        "        #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "        loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "        \n",
        "        #3. combine loss\n",
        "        loss = loss_lm + loss_nsp\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(epoch_loss / num_step))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model\n",
        "save_path = './model/bert.pt'\n",
        "torch.save([model.params, model.state_dict()], save_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference\n",
        "\n",
        "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the model and all its hyperparameters\n",
        "params, state = torch.load(save_path)\n",
        "model = BERT(**params, device=device).to(device)\n",
        "model.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'hull', 'travel', 'to', 'belgium', 'to', 'face', 'lokeren', 'in', 'europa', 'league', 'qualifying', 'round', 'steve', 'bruce', 'intends', 'to', 'make', 'lots', 'of', '[MASK]', 'to', 'the', 'side', 'who', 'beat', 'qpr', '10', 'but', 'he', 'cannot', 'change', 'the', 'whole', '11', 'ishinomaki', '[MASK]', 'only', 'has', 'a', '21man', '[MASK]', 'robert', 'snodgrass', 'will', 'not', 'feature', '[MASK]', 'the', '[MASK]', 'stadium', '[MASK]', 'hull', 'the', 'midfielder', 'has', 'been', 'ruled', '[MASK]', 'for', 'six', 'months', 'after', 'dislocating', 'his', 'knee', '[SEP]', 'roger', 'federer', 'and', 'rod', 'laver', 'delight', 'melbourne', 'chuckle', 'by', 'playing', 'a', 'few', 'rallies', 'federer', 'was', 'playing', 'jowilfried', 'tsonga', 'in', 'a', 'charity', 'match', 'to', 'raise', 'money', 'for', 'his', 'foundation', 'australian', 'laver', 'won', '11', 'grand', 'slam', 'titles', '[MASK]', 'his', '13year', 'playing', 'career', 'the', 'rod', 'laver', 'arena', 'in', 'melbourne', '[MASK]', 'host', 'to', 'the', 'australian', 'open', 'finals', '[SEP]']\n",
            "masked tokens (words) :  ['out', 'a', 'hull', 'at', 'as', 'he', 'titles', 'plays', 'for', 'crowd', 'squad', 'daknam', 'during', 'belgium', 'changes']\n",
            "masked tokens list :  [738, 25676, 14118, 28940, 18079, 16785, 9045, 305, 37322, 5827, 21579, 34724, 12191, 4956, 13472]\n",
            "predicted masked tokens (words) :  ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "predicted masked tokens list :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "1\n",
            "isNext :  False\n",
            "predict isNext :  True\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0][0]))\n",
        "input_ids = input_ids.to(device)\n",
        "segment_ids = segment_ids.to(device)\n",
        "masked_tokens = masked_tokens.to(device)\n",
        "masked_pos = masked_pos.to(device)\n",
        "isNext = isNext.to(device)\n",
        "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
        "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
        "\n",
        "#predict masked tokens\n",
        "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
        "logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy() \n",
        "#note that zero is padding we add to the masked_tokens\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
        "print('predicted masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
        "print('predicted masked tokens list : ', [pos for pos in logits_lm])\n",
        "\n",
        "#predict nsp\n",
        "logits_nsp = logits_nsp.data.cpu().max(1)[1][0].data.numpy()\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_nsp else False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying a bigger dataset should be able to see the difference."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
