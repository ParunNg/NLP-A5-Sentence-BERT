{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"swag\", \"regular\", split={\"train\": \"train\", \"validation\": \"validation\"})\n",
        "dataset = dataset.remove_columns(['video-id', 'fold-ind', 'sent1', 'sent2', 'gold-source'])  # remove all irrelavant fields as we will not be using them in our task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['startphrase', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
              "        num_rows: 73546\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['startphrase', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
              "        num_rows: 20006\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start Phrase: Members of the procession walk down the street holding small horn brass instruments. A drum line\n",
            "True Ending: passes by walking down the street playing their instruments.\n",
            "Label: 0\n"
          ]
        }
      ],
      "source": [
        "# Let's check the first startphrase and the true ending\n",
        "print(f\"Start Phrase: {dataset['train']['startphrase'][0]}\")\n",
        "print(f\"True Ending: {dataset['train']['ending0'][0]}\")\n",
        "print(f\"Label: {dataset['train']['label'][0]}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "### Tokenization and numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def transform_data(example):\n",
        "    start = re.sub(\"[.,!?\\\\-]\", '', example['startphrase'].lower())\n",
        "    true_label = example['label']\n",
        "\n",
        "    if np.random.random() < 0.5:\n",
        "        ending = re.sub(\"[.,!?\\\\-]\", '', example[f'ending{true_label}'].lower())\n",
        "        return {'start': start,\n",
        "                'ending': ending,\n",
        "                'isNext': True}\n",
        "    else:\n",
        "        ending_choice = [0, 1, 2, 3]\n",
        "        ending_choice.remove(true_label)\n",
        "        ending = re.sub(\"[.,!?\\\\-]\", '', example[f'ending{random.choice(ending_choice)}'].lower())\n",
        "        return {'start': start,\n",
        "                'ending': ending,\n",
        "                'isNext': False}\n",
        "    \n",
        "transformed_dataset = dataset.map(transform_data, remove_columns=['startphrase', 'ending0', 'ending1', 'ending2', 'ending3', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchtext\n",
        "\n",
        "# load the 'basic_english' tokenizer (other tokenizers can also be used such as 'spacy' and 'subword')\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "# create another field called 'tokens' which contains list of tokens\n",
        "tokenize_data = lambda example, tokenizer: {k+'_tokens': tokenizer(v) for (k,v) in example.items() if k != 'isNext'}\n",
        "tokenized_dataset = transformed_dataset.map(tokenize_data, remove_columns=['start', 'ending'], fn_kwargs={'tokenizer': tokenizer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    tokenized_dataset['train']['start_tokens']+tokenized_dataset['train']['ending_tokens']\n",
        ")\n",
        "vocab.insert_token('[PAD]', 0)\n",
        "vocab.insert_token('[CLS]', 1)\n",
        "vocab.insert_token('[SEP]', 2)\n",
        "vocab.insert_token('[MASK]', 3)\n",
        "vocab.insert_token('[UNK]', 4)\n",
        "vocab.set_default_index(vocab['[UNK]'])\n",
        "\n",
        "# save vocab_transform\n",
        "torch.save(vocab, './model/vocab')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loader\n",
        "\n",
        "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
        "\n",
        "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
        "\n",
        "2. **Segment embedding**\n",
        "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
        "\n",
        "3. **Masking**\n",
        "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
        "\n",
        "4. **Padding**\n",
        "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
        "\n",
        "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def get_data(dataset, vocab, max_mask, max_len):\n",
        "    data = []\n",
        "    \n",
        "    for example in tqdm(dataset):\n",
        "        # transform the list of tokens to list of token's indices based on our vocab\n",
        "        tokens_a = [vocab[token] for token in example['start_tokens']]\n",
        "        tokens_b = [vocab[token] for token in example['ending_tokens']]\n",
        "\n",
        "        #1. token embedding - add CLS and SEP\n",
        "        input_ids = [vocab['[CLS]']] + tokens_a + [vocab['[SEP]']] + tokens_b + [vocab['[SEP]']]\n",
        "\n",
        "        #2. segment embedding - which sentence is 0 and 1\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get all the pos excluding CLS and SEP\n",
        "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != vocab['[CLS]'] \n",
        "                                 and token != vocab['[SEP]']]\n",
        "        np.random.shuffle(candidates_masked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "\n",
        "        #simply loop and mask accordingly\n",
        "        for pos in candidates_masked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            rand_val = np.random.random()\n",
        "            if rand_val < 0.1:  #10% replace with random token\n",
        "                index = np.random.randint(5, len(vocab) - 1)  # random token should not involve [PAD], [CLS], [SEP], [MASK], [UNK]\n",
        "                input_ids[pos] = vocab[vocab.get_itos()[index]]\n",
        "            elif rand_val < 0.8:  #80 replace with [MASK]\n",
        "                input_ids[pos] = vocab['[MASK]']\n",
        "            else: \n",
        "                pass\n",
        "\n",
        "        #4. pad the sentence to the max length\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        #5. pad the mask tokens to the max length\n",
        "        if max_mask > n_pred:\n",
        "            n_pad = max_mask - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        instance = [input_ids, segment_ids, masked_tokens, masked_pos, [example['isNext']]]\n",
        "        instance = [torch.LongTensor(i) for i in instance]\n",
        "        data.append(instance)\n",
        "        \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "max_mask   = 5  #even though it does not reach 15% yet....maybe you can set this threshold\n",
        "max_len    = 512  #maximum length that my transformer will accept.....all sentence will be padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 73546/73546 [00:27<00:00, 2646.03it/s]\n",
            "100%|██████████| 20006/20006 [00:07<00:00, 2660.78it/s]\n"
          ]
        }
      ],
      "source": [
        "train_data = get_data(tokenized_dataset['train'], vocab, max_mask, max_len)\n",
        "val_data = get_data(tokenized_dataset['validation'], vocab, max_mask, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "for input_ids, segment_ids, masked_tokens, masked_pos, isNext in train_loader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([16, 512]),\n",
              " torch.Size([16, 512]),\n",
              " torch.Size([16, 5]),\n",
              " torch.Size([16, 5]),\n",
              " tensor([[0],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [0],\n",
              "         [0],\n",
              "         [0],\n",
              "         [1],\n",
              "         [0],\n",
              "         [1],\n",
              "         [0],\n",
              "         [0],\n",
              "         [1],\n",
              "         [0]]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model\n",
        "\n",
        "Recall that BERT only uses the encoder.\n",
        "\n",
        "BERT has the following components:\n",
        "\n",
        "- Embedding layers\n",
        "- Attention Mask\n",
        "- Encoder layer\n",
        "- Multi-head attention\n",
        "- Scaled dot product attention\n",
        "- Position-wise feed-forward network\n",
        "- BERT (assembling all the components)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k, device):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids, 'cpu').shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Encoder\n",
        "\n",
        "The encoder has two main components: \n",
        "\n",
        "- Multi-head Attention\n",
        "- Position-wise feed-forward network\n",
        "\n",
        "First let's make the wrapper called `EncoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the scaled dot attention, to be used inside the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, device):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the Multiheadattention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, device):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_k\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
        "        self.device = device\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
        "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the PoswiseFeedForwardNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
        "        super(BERT, self).__init__()\n",
        "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
        "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
        "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
        "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp\n",
        "    \n",
        "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the parameters first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model = BERT(n_layers, n_heads, d_model, d_ff, d_k, n_segments, len(vocab), max_len, device).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)  # BERT paper used 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, device):\n",
        "    epoch_loss = []\n",
        "    model.train()\n",
        "\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in tqdm(data, desc='Training: '):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        masked_tokens = masked_tokens.to(device)\n",
        "        masked_pos = masked_pos.to(device)\n",
        "        isNext = isNext.flatten().to(device)\n",
        "        logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "        #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "        #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "        #1. mlm loss\n",
        "        #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        #2. nsp loss\n",
        "        #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "        loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "        \n",
        "        #3. combine loss\n",
        "        loss = loss_lm + loss_nsp\n",
        "        epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, device):\n",
        "    epoch_loss = []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():  # set the 'requires_grad' to False to speed up computation and reduce memory cost\n",
        "        for input_ids, segment_ids, masked_tokens, masked_pos, isNext in tqdm(data, desc='Evaluate: '):\n",
        "            input_ids = input_ids.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            masked_tokens = masked_tokens.to(device)\n",
        "            masked_pos = masked_pos.to(device)\n",
        "            isNext = isNext.flatten().to(device)\n",
        "            logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "            #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "            #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "            #1. mlm loss\n",
        "            #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "            loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "            loss_lm = (loss_lm.float()).mean()\n",
        "            #2. nsp loss\n",
        "            #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "            loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "            \n",
        "            #3. combine loss\n",
        "            loss = loss_lm + loss_nsp\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [12:58<00:00,  5.90it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 14m 18s\n",
            "\tTrain Loss: 12.091\n",
            "\t Val. Loss: 7.560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:15<00:00,  5.78it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 14m 35s\n",
            "\tTrain Loss: 6.389\n",
            "\t Val. Loss: 6.093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [12:56<00:00,  5.92it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 14m 15s\n",
            "\tTrain Loss: 5.475\n",
            "\t Val. Loss: 5.619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:02<00:00,  5.88it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:20<00:00, 15.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 14m 22s\n",
            "\tTrain Loss: 5.074\n",
            "\t Val. Loss: 5.353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:02<00:00,  5.87it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 14m 22s\n",
            "\tTrain Loss: 4.838\n",
            "\t Val. Loss: 5.234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:04<00:00,  5.86it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:20<00:00, 15.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 14m 24s\n",
            "\tTrain Loss: 4.667\n",
            "\t Val. Loss: 5.144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:04<00:00,  5.86it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 14m 24s\n",
            "\tTrain Loss: 4.514\n",
            "\t Val. Loss: 5.113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:00<00:00,  5.89it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 14m 20s\n",
            "\tTrain Loss: 4.369\n",
            "\t Val. Loss: 5.106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:01<00:00,  5.88it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:19<00:00, 15.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 14m 21s\n",
            "\tTrain Loss: 4.261\n",
            "\t Val. Loss: 5.100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 4597/4597 [13:02<00:00,  5.87it/s]\n",
            "Evaluate: 100%|██████████| 1251/1251 [01:20<00:00, 15.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 14m 22s\n",
            "\tTrain Loss: 4.168\n",
            "\t Val. Loss: 5.168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "save_path = './model/bert.pt'\n",
        "num_epoch = 10\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    lr_scheduler.step(val_loss)\n",
        "\n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # save the model only when its validation loss is lower than all its predecessors\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save([model.params, model.state_dict()], save_path)  # save the model's parameters and state to a file\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(train_losses, label = 'train loss')\n",
        "ax.plot(val_losses, label = 'validation loss')\n",
        "plt.legend()\n",
        "ax.set_xlabel('updates')\n",
        "ax.set_ylabel('loss')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference\n",
        "\n",
        "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the model and all its hyperparameters\n",
        "params, state = torch.load(save_path)\n",
        "model = BERT(**params, device=device).to(device)\n",
        "model.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'the', '[UNK]', 'gets', '[MASK]', 'of', 'bed', 'and', 'prepares', 'coffe', 'leave', 'by', 'bathing', '[MASK]', 'the', 'bathroom', 'and', 'reading', 'the', 'newspaper', 'he', '[SEP]', 'rides', 'the', 'motorcycle', 'down', 'the', 'hall', 'and', 'into', 'the', '[MASK]', '[SEP]']\n",
            "masked tokens (words) :  ['out', 'using', 'to', 'elevator', 'reading']\n",
            "masked tokens list :  [32, 197, 9, 828, 889]\n",
            "predicted masked tokens (words) :  ['to', 'the', 'giving', 'to', 'reading']\n",
            "predicted masked tokens list :  [9, 5, 965, 9, 889]\n",
            "1\n",
            "isNext :  True\n",
            "predict isNext :  True\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "for input_ids, segment_ids, masked_tokens, masked_pos, isNext in val_loader:\n",
        "    break\n",
        "\n",
        "idx = 2\n",
        "input_ids = input_ids[idx].reshape(1, -1).to(device)\n",
        "segment_ids = segment_ids[idx].reshape(1, -1).to(device)\n",
        "masked_tokens = masked_tokens[idx].reshape(1, -1).to(device)\n",
        "masked_pos = masked_pos[idx].reshape(1, -1).to(device)\n",
        "isNext = isNext[idx].item()\n",
        "\n",
        "print([vocab.get_itos()[w.item()] for w in input_ids[0] if vocab.get_itos()[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
        "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
        "\n",
        "#predict masked tokens\n",
        "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
        "logits_lm = logits_lm.data.cpu().max(2)[1][0].data.numpy() \n",
        "#note that zero is padding we add to the masked_tokens\n",
        "print('masked tokens (words) : ',[vocab.get_itos()[pos.item()] for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predicted masked tokens (words) : ',[vocab.get_itos()[pos.item()] for pos in logits_lm if pos.item() != 0])\n",
        "print('predicted masked tokens list : ', [pos for pos in logits_lm if pos.item() != 0])\n",
        "\n",
        "#predict nsp\n",
        "logits_nsp = logits_nsp.data.cpu().max(1)[1][0].data.numpy()\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ', True if logits_nsp else False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
